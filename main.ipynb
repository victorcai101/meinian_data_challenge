{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "#the merge part is forked from mapodoufu\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "part_1 = pd.read_csv('../data/meinian_round1_data_part1_20180408.txt',sep='$')\n",
    "part_2 = pd.read_csv('../data/meinian_round1_data_part2_20180408.txt',sep='$')\n",
    "part_1_2 = pd.concat([part_1,part_2],axis = 0)\n",
    "part_1_2 = pd.DataFrame(part_1_2).sort_values('vid').reset_index(drop=True)\n",
    "begin_time = time.time()\n",
    "print('begin')\n",
    "# 重复数据的拼接操作\n",
    "def merge_table(df):\n",
    "    df['field_results'] = df['field_results'].astype(str)\n",
    "    if df.shape[0] > 1:\n",
    "        merge_df = \" \".join(list(df['field_results']))\n",
    "    else:\n",
    "        merge_df = df['field_results'].values[0]\n",
    "    return merge_df\n",
    "# 数据简单处理\n",
    "print('find_is_copy')\n",
    "print(part_1_2.shape)\n",
    "is_happen = part_1_2.groupby(['vid','table_id']).size().reset_index()\n",
    "# 重塑index用来去重\n",
    "is_happen['new_index'] = is_happen['vid'] + '_' + is_happen['table_id']\n",
    "is_happen_new = is_happen[is_happen[0]>1]['new_index']\n",
    "\n",
    "part_1_2['new_index'] = part_1_2['vid'] + '_' + part_1_2['table_id']\n",
    "\n",
    "unique_part = part_1_2[part_1_2['new_index'].isin(list(is_happen_new))]\n",
    "unique_part = unique_part.sort_values(['vid','table_id'])\n",
    "no_unique_part = part_1_2[~part_1_2['new_index'].isin(list(is_happen_new))]\n",
    "print('begin')\n",
    "part_1_2_not_unique = unique_part.groupby(['vid','table_id']).apply(merge_table).reset_index()\n",
    "part_1_2_not_unique.rename(columns={0:'field_results'},inplace=True)\n",
    "print('xxx')\n",
    "tmp = pd.concat([part_1_2_not_unique,no_unique_part[['vid','table_id','field_results']]])\n",
    "# 行列转换\n",
    "print('finish')\n",
    "tmp = tmp.pivot(index='vid',values='field_results',columns='table_id')\n",
    "#tmp.to_csv('./input/tmp.csv')\n",
    "print(tmp.shape)\n",
    "print('totle time',time.time() - begin_time)\n",
    "tmp.replace('未查',np.nan,inplace=True)\n",
    "tmp.replace('弃查',np.nan,inplace=True)\n",
    "import gc\n",
    "del part_1,part_2,part_1_2,is_happen,is_happen_new,unique_part,no_unique_part,part_1_2_not_unique\n",
    "gc.collect()\n",
    "#import pickle\n",
    "#with open('.\\\\input\\\\tmp.pk', 'wb') as f:\n",
    "#     pickle.dump(tmp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_feat = dict()\n",
    "\n",
    "data_feat['all_feat'] = set(tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_tmp_ditch = []\n",
    "for s in data_feat['all_feat']:\n",
    "    if tmp[s].isnull().mean() > 0.95:\n",
    "        feat_tmp_ditch.append(s)\n",
    "\n",
    "data_feat['ditch'] = set(feat_tmp_ditch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_tmp_words = []\n",
    "for s in data_feat['all_feat'] - data_feat['ditch']:\n",
    "    if tmp[s].dropna().str.len().mean() > 5:\n",
    "        feat_tmp_words.append(s)\n",
    "\n",
    "feat_tmp_words = set(feat_tmp_words) - set(['3301', '100014', '1474', '269011', '191', '1104', '669009', '809045', '669007', '1345'])\n",
    "\n",
    "data_feat['words'] = feat_tmp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_tmp_num_1 = []\n",
    "for s in data_feat['all_feat'] - data_feat['ditch'] - data_feat['words']:\n",
    "    try:\n",
    "        tmp[s].astype(float)\n",
    "    except:pass\n",
    "    else:\n",
    "        feat_tmp_num_1.append(s)\n",
    "\n",
    "data_feat['num'] = set(feat_tmp_num_1)\n",
    "\n",
    "feat_tmp_num_2 = [] # suspicious numerical features\n",
    "for s in data_feat['all_feat'] - data_feat['ditch'] - data_feat['words'] - data_feat['num']:\n",
    "    if tmp[s].dropna().str.contains('\\d').mean() > 0.8:\n",
    "        feat_tmp_num_2.append(s)\n",
    "\n",
    "data_feat['num_ish'] = set(feat_tmp_num_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_feat['cat'] = data_feat['all_feat'] - data_feat['ditch'] - data_feat['words'] - data_feat['num'] - data_feat['num_ish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = tmp[list(data_feat['num_ish'])]\n",
    "out.replace(r'．', '.', regex = True, inplace = True)\n",
    "out.replace(r'０', '0', regex = True, inplace = True)\n",
    "out.replace(r'１', '1', regex = True, inplace = True)\n",
    "out.replace(r'２', '2', regex = True, inplace = True)\n",
    "out.replace(r'３', '3', regex = True, inplace = True)\n",
    "out.replace(r'４', '4', regex = True, inplace = True)\n",
    "out.replace(r'５', '5', regex = True, inplace = True)\n",
    "out.replace(r'６', '6', regex = True, inplace = True)\n",
    "out.replace(r'７', '7', regex = True, inplace = True)\n",
    "out.replace(r'８', '8', regex = True, inplace = True)\n",
    "out.replace(r'９', '9', regex = True, inplace = True)\n",
    "out = out.replace(['/', r'^详见', r'^未做', r'^未查', r'^-+$', r'退检$'], np.nan, regex=True)\n",
    "out.replace(r'^<(.)*$', r'\\1', regex = True, inplace = True)\n",
    "out.replace(r'^>(.)*$', r'\\1', regex = True, inplace = True)\n",
    "out.replace(r'^阴性$', 0, regex = True, inplace = True)\n",
    "out.replace(r'^未见$', 0, regex = True, inplace = True)\n",
    "rep_fun = lambda x: str(np.nanmean([float(i) for i in x.group(0).split()]))\n",
    "\n",
    "for s in out.columns:\n",
    "    out.loc[:, s] = out.loc[:, s].str.replace(r'^(([+-]?[0-9]*[.]?[0-9]+)|nan) (([+-]?[0-9]*[.]?[0-9]+)|nan)$', rep_fun)\n",
    "    out.loc[:, s] = out.loc[:, s].str.replace(r'^(([+-]?[0-9]*[.]?[0-9]+)|nan) (([+-]?[0-9]*[.]?[0-9]+)|nan) (([+-]?[0-9]*[.]?[0-9]+)|nan)$', rep_fun)\n",
    "    out.loc[:, s] = out.loc[:, s].str.replace(r'^(([+-]?[0-9]*[.]?[0-9]+)|nan) (([+-]?[0-9]*[.]?[0-9]+)|nan) (([+-]?[0-9]*[.]?[0-9]+)|nan) (([+-]?[0-9]*[.]?[0-9]+)|nan)$', rep_fun)\n",
    "\n",
    "out.replace([r'^<=([+-]?[0-9]*[.]?[0-9]+)$', r'^>=([+-]?[0-9]*[.]?[0-9]+)$',\\\n",
    "            r'^＜([+-]?[0-9]*[.]?[0-9]+)$', r'^＞([+-]?[0-9]*[.]?[0-9]+)$', \\\n",
    "            r'^＜＝([+-]?[0-9]*[.]?[0-9]+)$', r'^＞＝([+-]?[0-9]*[.]?[0-9]+)$'], r'\\1', regex = True, inplace=True)\n",
    "\n",
    "out.replace(r'。', '.', regex = True, inplace = True)\n",
    "\n",
    "out.replace(r'^\\s*(\\S*)\\s*$', r'\\1', regex = True, inplace = True)\n",
    "\n",
    "out.replace([r'^(.+)S$', r'^(.+)\\.$'], r'\\1', regex = True, inplace = True)\n",
    "\n",
    "out.replace(r'^(\\d+)\\.\\.(\\d+)$', r'\\1.\\2', regex = True, inplace = True)\n",
    "\n",
    "out.replace(['+', '.'], np.nan, regex=False, inplace = True)\n",
    "\n",
    "out.replace(r'^([+-]?[0-9]*[.]?[0-9]+)-$', r'\\1', regex=True, inplace = True)\n",
    "\n",
    "out.replace(r'^([+-]?[0-9]*[.]?[0-9]+)kpa$', r'\\1', regex=True, inplace = True)\n",
    "\n",
    "# 2404\n",
    "out['2404'] = out['2404'].str.replace(r'^(\\d+) (\\S+)$', r'\\1')\n",
    "\n",
    "# 1325\n",
    "out['1325'][out['1325'].dropna()[out['1325'].dropna().str.match(r'义眼|光感|失明|弃查|指数|正常')].index] = np.nan\n",
    "\n",
    "out['1325']['0a67044b74d49b6d9e1a9a7c06a0731e'] = 0.8\n",
    "out['1325']['115e39811df6cdc8c70bd0d58f555558'] = 1.0\n",
    "out['1325']['1170d101f8f0dd7ab8c005b9fe0bdb68'] = 1.2\n",
    "out['1325']['76f813be607bf630e29045b07c6e1e1a'] = 1.5\n",
    "out['1325']['8096e0132a874a4181a7643fd5e3493a'] = np.nan\n",
    "out['1325']['a8b397c1c014e2db76ec8f360d680283'] = np.nan\n",
    "out['1325']['af5109d1a04fcdd4ff06bd42745bdfcc'] = np.nan\n",
    "out['1325']['cca0f6c8bfbd8d0ddf167083a3a29ecd'] = 1.2\n",
    "\n",
    "# 1815\n",
    "\n",
    "out['1815']['62bdd9d1540d90a89281ca820f4d69bb'] = np.nan\n",
    "\n",
    "# 2409\n",
    "\n",
    "out['2409'] = out['2409'].str.replace(r'^(.+)\\(.+\\)$', r'\\1')\n",
    "\n",
    "out['2409'] = out['2409'].str.replace(r'^(.+)%$', r'\\1')\n",
    "\n",
    "out['2409'] = out['2409'].str.replace(r'^(.+)% $', r'\\1')\n",
    "\n",
    "# 300009\n",
    "\n",
    "\n",
    "out['300009']['8a5390a401a3f13d03480f8340140148'] = 45.21\n",
    "\n",
    "# 1112\n",
    "\n",
    "\n",
    "out['1112'] = out['1112'].str.replace(r'^(.+) %$', r'\\1')\n",
    "\n",
    "# 269013\n",
    "\n",
    "\n",
    "rep_fun_2 = lambda x: str(np.nanmean([float(i) for i in x.group(0).split('-')]))\n",
    "out['269013'] = out['269013'].str.replace(r'^(\\d)-(\\d)$', rep_fun_2)\n",
    "\n",
    "# 300078\n",
    "\n",
    "out['300078']['662d00fbf6636f3b3c311189e99aac30'] = np.nan\n",
    "\n",
    "# 10004\n",
    "\n",
    "\n",
    "out['10004']['561416800096bedd9cf6fbad7cef0ef7'] = 3.7\n",
    "\n",
    "# A701\n",
    "out['A701']['d0b81951043aa245b46e3f110af60a7f'] = 6.3\n",
    "\n",
    "# 0424\n",
    "\n",
    "out['0424'] = out['0424'].str.replace(r'^\\D+(\\d+)$', r'\\1')\n",
    "out['0424'] = out['0424'].str.replace(r'^(\\d+)\\D+$', r'\\1')\n",
    "\n",
    "out['0424'] = out['0424'].str.replace('心动过缓', '60')\n",
    "out['0424'] = out['0424'].str.replace('心率正常', '80')\n",
    "out['0424'] = out['0424'].str.replace('未见异常', '80')\n",
    "out['0424'] = out['0424'].str.replace('正常', '80')\n",
    "out['0424'] = out['0424'].str.replace('窦性心动过缓', '60')\n",
    "out['0424'] = out['0424'].str.replace('窦性心动过速', '100')\n",
    "\n",
    "out['0424'] = out['0424'].str.replace('窦性60', '60') # 竟然不是整句匹配\n",
    "\n",
    "out['0424']['8c6551f10e8610fcadd93970fd9ff482'] = 75\n",
    "\n",
    "# 312\n",
    "\n",
    "out['312'] = out['312'].str.replace(r'(?:\\s|^)(\\d+)-(\\d+)(?:\\s|$)', lambda x: ' '+rep_fun_2(x)+' ')\n",
    "\n",
    "out['312'] = out['312'].str.replace(r'^(.+) $', r'\\1')\n",
    "out['312'] = out['312'].str.replace(r'^ (.+)$', r'\\1')\n",
    "\n",
    "out['312'] = out['312'].str.replace('未见', 'nan')\n",
    "\n",
    "out['312'] = out['312'].str.replace('+', '')\n",
    "out['312'] = out['312'].str.replace(r'^(.+) $', r'\\1')\n",
    "out['312'] = out['312'].str.replace(r'^ (.+)$', r'\\1')\n",
    "\n",
    "out['312'] = out['312'].str.replace(r'^(([+-]?[0-9]*[.]?[0-9]+)|nan) (([+-]?[0-9]*[.]?[0-9]+)|nan)$', rep_fun)\n",
    "\n",
    "# 191\n",
    "\n",
    "out['191'] = out['191'].str.replace('降脂后复查', 'nan')\n",
    "\n",
    "not_num = lambda this_feat:out[this_feat].dropna()[~out[this_feat].dropna().str.match('^(?:[+-]?([0-9]*[.])?[0-9]+|nan)$')]\n",
    "\n",
    "# 1320\n",
    "\n",
    "out['1320'] = out['1320'].str.replace(r' 正常.*$', '')\n",
    "\n",
    "out['1320'] = out['1320'].str.replace(r'mmHg$', '')\n",
    "\n",
    "out['1320'] = out['1320'].str.replace(r'^(\\d+)\\D+$', r'\\1')\n",
    "\n",
    "out['1320'] = out['1320'].str.replace(r'^.*正常.*$', '15')\n",
    "\n",
    "out['1320'] = out['1320'].str.replace(r'^.*高.*$', '20')\n",
    "\n",
    "out['1320'][not_num('1320').index] = 'nan'\n",
    "\n",
    "# 313\n",
    "\n",
    "out['313']['efc9d79c3cfca3c5eaef9ebce428bac2'] = 189\n",
    "\n",
    "# 300017\n",
    "out['300017']['f32fc7e0d65c3f0ef11dba36284b99f4'] = 1.496\n",
    "\n",
    "# 2403\n",
    "\n",
    "out['2403'] = out['2403'].str.replace(r'^(\\d+)\\s\\S+$', r'\\1')\n",
    "\n",
    "# 2372\n",
    "\n",
    "out['2372']['e50b5c308f0cb33351479f92667f933b'] = (2.79 + 2.20)/2\n",
    "\n",
    "# 300013\n",
    "\n",
    "out['300013']['53027913d6c15b82098658f3b358ffad'] = (6.31 + 0.45) / 2\n",
    "\n",
    "# 300076\n",
    "out['300076']['662d00fbf6636f3b3c311189e99aac30'] = np.nan\n",
    "\n",
    "# 192\n",
    "\n",
    "out['192']['f9d1b02f5579d4e73ac833775724fbc2'] = np.nan\n",
    "\n",
    "# 1319\n",
    "\n",
    "out['1319'] = out['1319'].str.replace(r' 正常范围.*$', '')\n",
    "\n",
    "out['1319'] = out['1319'].str.replace(r'mmHg$', '')\n",
    "\n",
    "out['1319'] = out['1319'].str.replace(r'^(\\d+)\\D+$', r'\\1')\n",
    "\n",
    "out['1319'] = out['1319'].str.replace(r'^.*正常.*$', '15')\n",
    "\n",
    "out['1319'] = out['1319'].str.replace(r'^.*高.*$', '20')\n",
    "\n",
    "out['1319'][not_num('1319').index] = 'nan'\n",
    "\n",
    "## 1322\n",
    "out['1322'][out['1322'].dropna()[out['1322'].dropna().str.match(r'义眼|光感|失明|弃查|指数|正常|无光感|手动|因无法配合不能检测|未要求检查')].index] = np.nan\n",
    "\n",
    "out['1322'][not_num('1322').index] = ['0.9', 'nan', '0.8', 'nan', '1.2', '1.2', 'nan', '0.8', '1.0', 'nan', '0.9',\\\n",
    "                                     'nan', 'nan']\n",
    "\n",
    "## 1326\n",
    "\n",
    "out['1326'][out['1326'].dropna()[out['1326'].dropna().str.match(r'义眼|光感|失明|弃查|指数|正常|无光感|手动|因无法配合不能检测|未要求检查')].index] = np.nan\n",
    "\n",
    "out['1326'][not_num('1326').index] = ['1.2', '1.0', '1.5', '1.38', 'nan', 'nan', '0.85', 'nan', '1.2', 'nan', '1.0']\n",
    "\n",
    "## 1321\n",
    "\n",
    "out['1321'][out['1321'].dropna()[out['1321'].dropna().str.match(r'义眼|光感|失明|弃查|指数|正常|无光感|手动|因无法配合不能检测|未要求检查')].index] = np.nan\n",
    "\n",
    "out['1321'][not_num('1321').index] = ['0.6', 'nan', '0.8', 'nan', '1.5', '1.2', 'nan', \\\n",
    "                                    '0.6', '1.0', 'nan', '1.0', '0.55', 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_ish = out.astype(float)\n",
    "num_ish = (num_ish - num_ish.mean(axis = 0))/ num_ish.std(axis = 0)\n",
    "num_ish.columns = [x + '*' for x in num_ish.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_word = tmp[list(data_feat['words'])]\n",
    "l_cat = tmp[list(data_feat['cat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "jieba.load_userdict(\"../code/user_dict.py\")\n",
    "MAX_FEATURE=50\n",
    "\n",
    "# stop words are the most common words in a language\n",
    "# which should be filtered out\n",
    "def stopwordslist(filepath):\n",
    "    stopword = [line.strip() for line in open(filepath, 'rb').readlines()]\n",
    "    return stopword\n",
    "stopwords = stopwordslist('../code/stop_words.py')\n",
    "\n",
    "# word segmentation using jieba\n",
    "def seg_sentence(sentence):\n",
    "    if (sentence is None) or (sentence != sentence): return \"\"\n",
    "    try:\n",
    "        sentence_seged = jieba.cut(sentence.strip())\n",
    "        # sentence_seged = [w.encode('utf8') for w in sentence_seged]\n",
    "        sentence_seged = [w for w in sentence_seged if w not in stopwords]\n",
    "        sentence_seged = \" \".join(sentence_seged)\n",
    "        return sentence_seged\n",
    "    except:\n",
    "        print(sentence)\n",
    "        return \"\"\n",
    "\n",
    "def word2tfidf(sentlist,min_df,max_df):\n",
    "    vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b',min_df = min_df, max_df =max_df, max_features = MAX_FEATURE)\n",
    "    transformer = TfidfTransformer()\n",
    "    count = vectorizer.fit_transform(sentlist)\n",
    "    tf_idf = transformer.fit_transform(count)\n",
    "    # print(len(vectorizer.get_feature_names()))\n",
    "    # print(vectorizer.get_feature_names())\n",
    "    return tf_idf.toarray().tolist()\n",
    "\n",
    "\n",
    "\n",
    "# 处理word类型的数据\n",
    "print(\"[PROCESS] Word file loaded, shape = \" + str(l_word.shape))\n",
    "dimension = 0\n",
    "for i in range(l_word.shape[1]):\n",
    "    if i%10==0: print(\"Word file \" + str(i) +\", dimention = \" + str(dimension))\n",
    "    word_list = l_word.iloc[:,i].values\n",
    "    word_list = [seg_sentence(sent) for sent in word_list]\n",
    "    tfidf = word2tfidf(word_list,0.00002,0.50)\n",
    "    dimension += len(tfidf[0])\n",
    "    l_word.iloc[:, i] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "word_2_label = []\n",
    "for col in range(l_word.shape[1]):\n",
    "    print(col)\n",
    "    vec_list = []\n",
    "    for i in range(l_word.shape[0]):\n",
    "        vec_list.append(l_word.iloc[i,col])\n",
    "    vec_array = np.array(vec_list)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "    kmeans.fit(vec_array)\n",
    "    word_2_label.append(pd.DataFrame(kmeans.predict(vec_array)))\n",
    "word_label = pd.concat(word_2_label,axis=1)\n",
    "word_label.columns = l_word.columns\n",
    "word_label.index = l_word.index\n",
    "del word_2_label,vec_list,vec_array,l_word\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_cat = tmp[list(data_feat['cat'])]\n",
    "print(\"[PROCESS] Word file transformed, dimention = \" + str(dimension))\n",
    "# 处理cat类型的数据\n",
    "print(\"[PROCESS] Cat file loaded, shape = \" + str(l_cat.shape))\n",
    "dimension = 0\n",
    "for i in range(l_cat.shape[1]):\n",
    "    if i%10==0: print(\"Word file \" + str(i) +\", dimention = \" + str(dimension))\n",
    "    word_list = l_cat.iloc[:,i].values\n",
    "    word_list = [seg_sentence(sent) for sent in word_list]\n",
    "    try:\n",
    "        tfidf = word2tfidf(word_list,0.0,1.00)\n",
    "    except:\n",
    "        print(i)\n",
    "    dimension += len(tfidf[0])\n",
    "    l_cat.iloc[:, i] = tfidf\n",
    "\n",
    "print(\"[PROCESS] Cat file transformed, dimention = \" + str(dimension))\n",
    "\n",
    "cat_2_label = []\n",
    "for col in range(l_cat.shape[1]):\n",
    "    print(col)\n",
    "    vec_list = []\n",
    "    for i in range(l_cat.shape[0]):\n",
    "        vec_list.append(l_cat.iloc[i,col])\n",
    "    vec_array = np.array(vec_list)\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "    kmeans.fit(vec_array)\n",
    "    cat_2_label.append(pd.DataFrame(kmeans.predict(vec_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_label = pd.concat(cat_2_label,axis=1)\n",
    "cat_label.columns = l_cat.columns\n",
    "cat_label.index = l_cat.index\n",
    "\n",
    "del cat_2_label,vec_list,vec_array,l_cat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_ish = num_ish.fillna(num_ish.mean())\n",
    "num_list = []\n",
    "for i in data_feat['num']:\n",
    "    num_list.append(i)\n",
    "num_X = tmp[num_list]\n",
    "num_X_list = []\n",
    "for i in range(num_X.shape[1]):\n",
    "    num_X_list.append(num_X.iloc[:,i].astype(float, inplace=True))\n",
    "num_X = pd.concat(num_X_list,axis = 1)\n",
    "num_X.fillna(num_X.mean(),inplace = True)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "std_num = pd.DataFrame(scaler.fit_transform(num_X))\n",
    "std_num.index = num_X.index\n",
    "std_num.columns = num_X.columns\n",
    "std_num_X = pd.concat([std_num,num_ish],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cat = pd.concat([word_label,cat_label],axis=1)\n",
    "all_X = pd.concat([all_cat,std_num_X],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = {'vid':str,'收缩压':object,'舒张压':object,'血清甘油三酯':object,'血清高密度脂蛋白':object,'血清低密度脂蛋白':object}\n",
    "train_data = pd.read_csv('..\\\\data\\\\meinian_round1_train_20180408.csv',encoding='gb2312',dtype=dtype)\n",
    "train_data = train_data.set_index('vid')\n",
    "train_data.replace('未查',np.nan,inplace=True)\n",
    "train_data.replace('弃查',np.nan,inplace=True)\n",
    "train_data.iloc[:,2] = train_data.iloc[:,2].str.strip('轻度乳糜').str.strip('+').str.strip('>').str.strip('=').str.strip('-')\n",
    "train_data.iloc[:,3] = train_data.iloc[:,3].str.strip('+').str.strip('>').str.strip('=').str.strip('-')\n",
    "train_data.iloc[:,4] = train_data.iloc[:,4].str.strip('+').str.strip('>').str.strip('=').str.strip('-')\n",
    "train_data.replace('2.2.8','2.28',inplace=True)\n",
    "train_data.replace('<=5.0',5.0,inplace=True)\n",
    "train_data.replace('＞＝１.０３０',1.030,inplace=True)\n",
    "train_data.replace('>=1.030',1.030,inplace=True)\n",
    "train_data.replace('未做',np.nan,inplace=True)\n",
    "train_data.replace('阴性',np.nan,inplace=True)\n",
    "train_data.replace('1.015.',1.015,inplace=True)\n",
    "train_data.replace('8.53.',8.53,inplace=True)\n",
    "train_data.replace('2.1.',2.1,inplace=True)\n",
    "train_data.replace('---',np.nan,inplace=True)\n",
    "train_data.replace('16.2-',16.1999,inplace=True)\n",
    "train_data.replace('189 脂血',189,inplace=True)\n",
    "train_data.replace('-',np.nan,inplace=True)\n",
    "train_data.replace('降脂后复查',np.nan,inplace=True)\n",
    "train_data.replace('无',np.nan,inplace=True)\n",
    "train_data.replace('3。89',3.89,inplace=True)\n",
    "train_data.replace('16.7.07',16.7,inplace=True)\n",
    "train_data.replace('',np.nan,inplace=True)\n",
    "train_data.replace('77..21',77.21,inplace=True)\n",
    "train_data.replace('0-5 3',3,inplace=True)\n",
    "train_data.replace('6-1',6.1,inplace=True)\n",
    "train_data.replace('未见',np.nan,inplace=True)\n",
    "train_data.replace('5..0',5.0,inplace=True)\n",
    "train_data.replace('详见报告单',np.nan,inplace=True)\n",
    "train_data.replace('Ⅱ',2,inplace=True)\n",
    "train_data.replace('Ⅲ',3,inplace=True)\n",
    "train_data.replace('Ⅰ',1,inplace=True)\n",
    "train_data.replace('Ⅲ度',3,inplace=True)\n",
    "train_data.replace('中度',3,inplace=True)\n",
    "train_data.replace('Ⅳ',4,inplace=True)\n",
    "train_data.replace('Ⅱ度',2,inplace=True)\n",
    "train_data.replace('未见异常',np.nan,inplace=True)\n",
    "train_data.replace('III',3,inplace=True)\n",
    "train_data.replace('正常',np.nan,inplace=True)\n",
    "train_data.replace('见TCT',np.nan,inplace=True)\n",
    "train_data.replace('iii°',3,inplace=True)\n",
    "train_data.replace('微混',1,inplace=True)\n",
    "train_data.replace('ii°',2,inplace=True)\n",
    "train_data.replace('II',2,inplace=True)\n",
    "train_data.replace('II',2,inplace=True)\n",
    "train_data = train_data.astype(float,inplace=True)\n",
    "train_data.replace(0,np.nan,inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "train_data = np.log10(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_data.join(all_X)\n",
    "test_vid = pd.read_csv('..\\\\data\\\\meinian_round1_test_b_20180505.csv',encoding = 'gb2312')\n",
    "test_vid.set_index('vid',inplace = True)\n",
    "X_test = test_vid.join(all_X).iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data,test_vid,num_X,std_num_X,all_cat,all_X,cat_label,word_label,tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "params = {\n",
    "    'objective': 'regression_l2',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric' : 'mse',\n",
    "    'num_leaves': 80,\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators':1000,\n",
    "    'feature_fraction': 0.3,\n",
    "    'bagging_freq': 2,\n",
    "    'num_threads': 4,\n",
    "    'reg_alpha':0.9,\n",
    "    'reg_lambda':0.0005,\n",
    "    'max_bin':100\n",
    "}\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "mse_list = []\n",
    "model_list = []\n",
    "#bagging_round = 100\n",
    "bagging_round = 1\n",
    "for rnd in range(bagging_round):\n",
    "    print('%d round'%(rnd+1))\n",
    "    seed = randrange(1,10000,1)\n",
    "    print('seed = ',seed)\n",
    "    X_train, X_cv, y_train, y_cv = train_test_split(train.iloc[:,5:], train.iloc[:,0:5], test_size=0.25, random_state=seed)\n",
    "    MAX_ROUNDS = 100000\n",
    "    val_pred = []\n",
    "    test_pred = []\n",
    "    cat_var = range(0,163)\n",
    "    for i in range(5):\n",
    "        print('%d round'%(rnd+1))\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Step %d\" % (i+1))\n",
    "        print(\"=\" * 50)\n",
    "        #dtrain = lgb.Dataset(X_train, label = y_train.iloc[:,i])\n",
    "        dtrain = lgb.Dataset(X_train, label = y_train.iloc[:,i],categorical_feature=cat_var)\n",
    "        dval = lgb.Dataset(X_cv, label = y_cv.iloc[:,i],reference=dtrain,categorical_feature=cat_var)\n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "            valid_sets=[dtrain, dval],categorical_feature=cat_var,early_stopping_rounds=50)\n",
    "        val_pred.append(bst.predict(X_cv, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "        test_pred.append(bst.predict(X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "        model_list.append(bst)\n",
    "    mse = 0\n",
    "    for i in range(len(val_pred)):\n",
    "        mse = mse + mean_squared_error(y_cv.iloc[:,i],val_pred[i].reshape(-1,1))/5\n",
    "    print('cv mse:',mse)\n",
    "    mse_list.append(mse)\n",
    "    test_exp = []\n",
    "    for i in range(len(test_pred)):\n",
    "        test_exp.append(pd.DataFrame(10**test_pred[i]))\n",
    "    test_exp = pd.concat(test_exp,axis=1)\n",
    "    test_exp.columns = ['收缩压', '舒张压', '血清甘油三酯', '血清高密度脂蛋白', '血清低密度脂蛋白']\n",
    "    test_exp.index = X_test.index\n",
    "    result_list.append(test_exp)\n",
    "a = result_list[0]/bagging_round\n",
    "for n in range(bagging_round-1):\n",
    "    a = a+result_list[n+1]/bagging_round\n",
    "import datetime\n",
    "a.to_csv('..\\\\submit\\\\submit_'+datetime.datetime.now().strftime('%Y%m%d_%H%M%S')+'.csv',header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = ['Systolic Pressure','Diastolic Pressure','Triglyceride','S-HDL','S-LDL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_train.shape[1]):\n",
    "    lgb.plot_importance(model_list[i],max_num_features=20)\n",
    "    plt.title(name_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv('E:\\\\Data Science Laboratory\\\\kaggle\\\\train_cleaned_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
